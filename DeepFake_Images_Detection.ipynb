{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MXdkiO6SMMsO"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip /content/drive/MyDrive/Kaggle/140k-real-and-fake-faces.zip"
      ],
      "metadata": {
        "id": "ZuD17AFQMN4m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow.keras\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras import regularizers\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Lambda, concatenate\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator, load_img\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import (Conv2D,Conv3D, BatchNormalization, Activation, MaxPooling2D, MaxPooling3D, GlobalAveragePooling2D, \n",
        "                          Dense, Flatten, Dropout)\n",
        "from tensorflow.keras.layers import  Dropout , BatchNormalization , Dense\n",
        "from tensorflow.keras.optimizers import RMSprop, Adam, SGD\n",
        "from tensorflow.keras.callbacks import Callback , ReduceLROnPlateau , ModelCheckpoint, CSVLogger\n",
        "from sklearn.metrics import cohen_kappa_score, accuracy_score , f1_score, recall_score, precision_score\n",
        "from tensorflow.keras.losses import categorical_crossentropy as logloss\n",
        "from tensorflow.keras.metrics import categorical_accuracy\n",
        "import numpy as np \n",
        "import pandas as pd \n",
        "from sklearn.metrics import classification_report , confusion_matrix\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "import os\n",
        "import cv2\n",
        "from PIL import Image\n",
        "import scipy\n",
        "import tensorflow as tf\n",
        "from sklearn import metrics\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tqdm import tqdm"
      ],
      "metadata": {
        "id": "nPdMXXOzrMKq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_datagen = ImageDataGenerator(rescale=1./255,\n",
        "                                   horizontal_flip=True,\n",
        "                                   vertical_flip=True,\n",
        "                                   rotation_range=360,\n",
        "                                   shear_range=0.2,\n",
        "                                   zoom_range=0.2\n",
        "                                  )\n",
        "training_set= train_datagen.flow_from_directory('/content/real_vs_fake/real-vs-fake/train',\n",
        "                                                 class_mode='binary',\n",
        "                                                 shuffle=True,\n",
        "                                                 target_size=(256,256),\n",
        "                                                 batch_size=64\n",
        "                                                )"
      ],
      "metadata": {
        "id": "1Mz7cD1iMYEH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "OB_fijcyMcS_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_datagen = ImageDataGenerator(rescale=1./255\n",
        "                                  )\n",
        "testing_set = test_datagen.flow_from_directory('/content/real_vs_fake/real-vs-fake/test',\n",
        "                                                 class_mode='binary',\n",
        "                                                 shuffle=True,\n",
        "                                                 target_size=(256,256),\n",
        "                                                 batch_size=64\n",
        "                                                )"
      ],
      "metadata": {
        "id": "L3k6pVdBoV8V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "valid_datagen = ImageDataGenerator(rescale=1./255\n",
        "                                  )\n",
        "valid_set = valid_datagen.flow_from_directory('/content/real_vs_fake/real-vs-fake/valid',\n",
        "                                                 class_mode='binary',\n",
        "                                                 shuffle=True,\n",
        "                                                 target_size=(256,256),\n",
        "                                                 batch_size=64\n",
        "                                                )"
      ],
      "metadata": {
        "id": "ArjcgUzNYjL2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "training_set.class_indices"
      ],
      "metadata": {
        "id": "E2xtkr5J0ZpW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "CNN = tf.keras.models.Sequential()\n",
        "CNN.add(tf.keras.layers.Conv2D(filters=8, kernel_size=3, activation='LeakyReLU', input_shape=[256,256,3]))\n",
        "CNN.add(BatchNormalization())\n",
        "\n",
        "CNN.add(tf.keras.layers.Conv2D(filters=16, kernel_size=3, activation='LeakyReLU'))\n",
        "CNN.add(tf.keras.layers.Conv2D(filters=16, kernel_size=3, activation='LeakyReLU'))\n",
        "CNN.add(BatchNormalization())\n",
        "CNN.add(tf.keras.layers.AveragePooling2D(pool_size=2, strides=2))\n",
        "\n",
        "CNN.add(tf.keras.layers.Conv2D(filters=32, kernel_size=3, activation='LeakyReLU'))\n",
        "CNN.add(tf.keras.layers.Conv2D(filters=32, kernel_size=3, activation='LeakyReLU'))\n",
        "CNN.add(tf.keras.layers.Conv2D(filters=32, kernel_size=3, activation='LeakyReLU'))\n",
        "CNN.add(BatchNormalization())\n",
        "CNN.add(tf.keras.layers.AveragePooling2D(pool_size=2, strides=2))\n",
        "\n",
        "CNN.add(tf.keras.layers.Conv2D(filters=64, kernel_size=3, activation='LeakyReLU'))\n",
        "CNN.add(tf.keras.layers.Conv2D(filters=64, kernel_size=3, activation='LeakyReLU'))\n",
        "CNN.add(tf.keras.layers.Conv2D(filters=64, kernel_size=3, activation='LeakyReLU'))\n",
        "CNN.add(tf.keras.layers.Conv2D(filters=64, kernel_size=3, activation='LeakyReLU'))\n",
        "CNN.add(BatchNormalization())\n",
        "CNN.add(tf.keras.layers.AveragePooling2D(pool_size=2, strides=2))\n",
        "\n",
        "#Max\n",
        "CNN.add(tf.keras.layers.Conv2D(filters=128, kernel_size=5, activation='LeakyReLU'))\n",
        "CNN.add(BatchNormalization())\n",
        "CNN.add(tf.keras.layers.MaxPooling2D(pool_size=2, strides=2))\n",
        "#Max\n",
        "CNN.add(tf.keras.layers.Conv2D(filters=256, kernel_size=5, activation='LeakyReLU'))\n",
        "CNN.add(BatchNormalization())\n",
        "CNN.add(tf.keras.layers.MaxPooling2D(pool_size=2, strides=2))\n",
        "\n",
        "CNN.add(tf.keras.layers.Flatten())\n",
        "CNN.add(Dropout(0.5))\n",
        "\n",
        "\n",
        "CNN.add(tf.keras.layers.Dense(units=32, activation='LeakyReLU'))\n",
        "CNN.add(Dropout(0.5))\n",
        "\n",
        "CNN.add(tf.keras.layers.Dense(units=16, activation='LeakyReLU'))\n",
        "CNN.add(Dropout(0.5))\n",
        "\n",
        "CNN.add(tf.keras.layers.Dense(units=16, activation='LeakyReLU'))\n",
        "CNN.add(Dropout(0.5))\n",
        "CNN.add(tf.keras.layers.Dense(units=1, activation='sigmoid'))\n"
      ],
      "metadata": {
        "id": "fSxZ48Eb7sbq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "CNN.compile(optimizer='adam', metrics=['accuracy'], loss='binary_crossentropy')"
      ],
      "metadata": {
        "id": "8w-N9rl30gRQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping\n",
        "\n",
        "# Define the paths for saving the best model weights and early stopping patience\n",
        "checkpoint_path = \"/content/drive/MyDrive/CNN Epoch.h5\"\n",
        "early_stopping_patience = 3\n",
        "\n",
        "# Define the ModelCheckpoint callback\n",
        "checkpoint_callback = ModelCheckpoint(\n",
        "    checkpoint_path,\n",
        "    monitor='val_loss',\n",
        "    save_best_only=False,\n",
        "    mode=\"min\",\n",
        "    verbose=3\n",
        ")\n",
        "reduce_lr = ReduceLROnPlateau(monitor='val_loss', \n",
        "                              factor=0.2, \n",
        "                              patience=3, \n",
        "                              verbose=3, \n",
        "                              min_delta=0.0001\n",
        "                             )\n",
        "# Define the EarlyStopping callback\n",
        "early_stopping_callback = EarlyStopping(\n",
        "    patience=early_stopping_patience,\n",
        "    monitor=\"val_loss\",\n",
        "    mode=\"min\",\n",
        "    verbose=3\n",
        ")"
      ],
      "metadata": {
        "id": "oqTkNx8HrpV_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "history =CNN.fit(training_set, batch_size=64, epochs=10, validation_data=(valid_set), callbacks=[early_stopping_callback ,checkpoint_callback,reduce_lr ])"
      ],
      "metadata": {
        "id": "sxBGDxIrYM5h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fig = plt.figure()\n",
        "plt.plot(history.history['accuracy'], color='teal', label='accuracy')\n",
        "plt.plot(history.history['val_accuracy'], color='orange', label='val_accuracy')\n",
        "fig.suptitle('Accuracy', fontsize=20)\n",
        "plt.legend(loc=\"lower right\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "Iiyrxb3uizfr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fig = plt.figure()\n",
        "plt.plot(history.history['loss'], color='teal', label='loss')\n",
        "plt.plot(history.history['val_loss'], color='orange', label='val_loss')\n",
        "fig.suptitle('Loss', fontsize=20)\n",
        "plt.legend(loc=\"upper left\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "sMaPC9qavsTP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate the model on the test data\n",
        "loss, accuracy = CNN.evaluate(testing_set)\n",
        "# Print the loss and accuracy\n",
        "print(f\"Loss: {loss}\")\n",
        "print(f\"Accuracy: {accuracy}\")"
      ],
      "metadata": {
        "id": "lYiEnVcDytLE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Proposed CNN model \n",
        "model = tf.keras.models.Sequential()\n",
        "model.add(tf.keras.layers.Conv2D(filters=8, kernel_size=3, activation='ReLU', input_shape=[256,256,3]))\n",
        "model.add(BatchNormalization())\n",
        "\n",
        "model.add(tf.keras.layers.Conv2D(filters=32, kernel_size=3, activation='ReLU', padding=\"same\"))\n",
        "model.add(BatchNormalization())\n",
        "model.add(tf.keras.layers.Conv2D(filters=32, kernel_size=3, activation='ReLU', padding=\"same\"))\n",
        "model.add(BatchNormalization())\n",
        "model.add(tf.keras.layers.Conv2D(filters=32, kernel_size=3, activation='ReLU', padding=\"same\"))\n",
        "model.add(BatchNormalization())\n",
        "model.add(tf.keras.layers.Conv2D(filters=32, kernel_size=3, activation='ReLU', padding=\"same\"))\n",
        "model.add(BatchNormalization())\n",
        "model.add(tf.keras.layers.MaxPooling2D(pool_size=2, strides=2))\n",
        "model.add(Dropout(0.25))\n",
        "\n",
        "model.add(tf.keras.layers.Conv2D(filters=64, kernel_size=3, activation='ReLU', padding=\"same\"))\n",
        "model.add(BatchNormalization())\n",
        "model.add(tf.keras.layers.Conv2D(filters=64, kernel_size=3, activation='ReLU', padding=\"same\"))\n",
        "model.add(BatchNormalization())\n",
        "model.add(tf.keras.layers.Conv2D(filters=64, kernel_size=3, activation='ReLU', padding=\"same\"))\n",
        "model.add(BatchNormalization())\n",
        "model.add(tf.keras.layers.Conv2D(filters=64, kernel_size=3, activation='ReLU', padding=\"same\"))\n",
        "model.add(BatchNormalization())\n",
        "model.add(tf.keras.layers.MaxPooling2D(pool_size=2, strides=2))\n",
        "model.add(Dropout(0.25))\n",
        "\n",
        "model.add(tf.keras.layers.Conv2D(filters=128, kernel_size=3, activation='ReLU', padding=\"same\"))\n",
        "model.add(BatchNormalization())\n",
        "model.add(tf.keras.layers.Conv2D(filters=128, kernel_size=3, activation='ReLU', padding=\"same\"))\n",
        "model.add(BatchNormalization())\n",
        "model.add(tf.keras.layers.Conv2D(filters=128, kernel_size=3, activation='ReLU', padding=\"same\"))\n",
        "model.add(BatchNormalization())\n",
        "model.add(tf.keras.layers.Conv2D(filters=128, kernel_size=3, activation='ReLU', padding=\"same\"))\n",
        "model.add(BatchNormalization())\n",
        "model.add(tf.keras.layers.MaxPooling2D(pool_size=2, strides=2))\n",
        "model.add(Dropout(0.5))\n",
        "\n",
        "\n",
        "\n",
        "#Flattend\n",
        "model.add(tf.keras.layers.Flatten())\n",
        "\n",
        "model.add(layers.Dense(128, activation=\"relu\"))\n",
        "model.add(layers.BatchNormalization())\n",
        "model.add(layers.Dropout(0.5))\n",
        "model.add(layers.Dense(64, activation=\"relu\"))\n",
        "model.add(layers.BatchNormalization())\n",
        "model.add(layers.Dropout(0.5))\n",
        "\n",
        "model.add(tf.keras.layers.Dense(units=1, activation='sigmoid'))\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "nLSua-1yUpw2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(optimizer='adam', metrics=['accuracy'], loss='binary_crossentropy')"
      ],
      "metadata": {
        "id": "HRbZ63vnytH0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping\n",
        "\n",
        "# Define the paths for saving the best model weights and early stopping patience\n",
        "checkpoint_path = \"/content/drive/MyDrive/NEW MODEL EPOCH.h5\"\n",
        "\n",
        "\n",
        "# Define the ModelCheckpoint callback\n",
        "checkpoint_callback = ModelCheckpoint(\n",
        "    checkpoint_path,\n",
        "    monitor='val_loss',\n",
        "    save_best_only=False,\n",
        "    mode=\"min\",\n",
        "    verbose=3\n",
        ")\n",
        "reduce_lr = ReduceLROnPlateau(monitor='val_loss', \n",
        "                              factor=0.2, \n",
        "                              patience=3, \n",
        "                              verbose=3, \n",
        "                              min_delta=0.0001\n",
        "                             )\n",
        "# Define the EarlyStopping callback\n",
        "early_stopping_callback = EarlyStopping(\n",
        "    patience=3,\n",
        "    monitor=\"val_loss\",\n",
        "    mode=\"min\",\n",
        "    verbose=3\n",
        ")"
      ],
      "metadata": {
        "id": "OweYJ9pQUpqX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "history1 =model.fit(training_set, batch_size=64, epochs=10, validation_data=(valid_set), callbacks=[early_stopping_callback ,checkpoint_callback,reduce_lr ])"
      ],
      "metadata": {
        "id": "-JCQizOoytCr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fig = plt.figure()\n",
        "plt.plot(history1.history['accuracy'], color='teal', label='accuracy')\n",
        "plt.plot(history1.history['val_accuracy'], color='orange', label='val_accuracy')\n",
        "fig.suptitle('Accuracy', fontsize=20)\n",
        "plt.legend(loc=\"lower right\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "Ew8dTasRytAL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fig = plt.figure()\n",
        "plt.plot(history1.history['loss'], color='teal', label='loss')\n",
        "plt.plot(history1.history['val_loss'], color='orange', label='val_loss')\n",
        "fig.suptitle('Loss', fontsize=20)\n",
        "plt.legend(loc=\"upper left\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "cykkkMybys9t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate the model on the test data\n",
        "loss, accuracy = CNN.evaluate(testing_set)\n",
        "# Print the loss and accuracy\n",
        "print(f\"Loss: {loss}\")\n",
        "print(f\"Accuracy: {accuracy}\")"
      ],
      "metadata": {
        "id": "G0_uJfH9an0R"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}